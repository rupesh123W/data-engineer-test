import os
import shutil
import uuid
from pyspark.sql import DataFrame
from pyspark.sql.functions import trim, regexp_replace, when, expr, isnan
from spark_utils import init_spark


# ----------------------------
# Safe column wrapper
# ----------------------------
def safe_col(cname: str):
    """Always wrap column names with backticks for Spark-safe usage."""
    return expr(f"`{cname}`")


# ----------------------------
# Data Quality Checks
# ----------------------------
def data_quality_checks(df: DataFrame, key_columns=None, numeric_cols=None) -> None:
    """Run basic data quality checks on the DataFrame."""
    print("\n=== Data Quality Checks ===")

    # 1. Null / blank values
    for col_name, dtype in df.dtypes:
        if dtype == "string":
            null_count = df.filter(
                safe_col(col_name).isNull() | (trim(safe_col(col_name)) == "")
            ).count()
            if null_count > 0:
                print(f"[WARN] Column '{col_name}' has {null_count} null/blank values")

    # 2. Duplicate checks
    if key_columns:
        dup_count = df.count() - df.dropDuplicates(key_columns).count()
        if dup_count > 0:
            print(f"[WARN] Found {dup_count} duplicate rows based on {key_columns}")
        else:
            print(f"[INFO] No duplicates found on keys {key_columns}")

    # 3. Numeric validation
    if numeric_cols:
        for col_name in numeric_cols:
            if col_name in df.columns:
                nulls = df.filter(
                    safe_col(col_name).isNull() | isnan(safe_col(col_name))
                ).count()
                if nulls > 0:
                    print(f"[WARN] Numeric column '{col_name}' has {nulls} NULL/NaN values")
                else:
                    print(f"[INFO] Numeric column '{col_name}' passed validation")

    print("=== Data Quality Checks Completed ===\n")


# ----------------------------
# Save Outputs (Windows-safe)
# ----------------------------
def save_outputs(df: DataFrame, output_parquet: str, output_csv: str) -> None:
    """Save DataFrame into Parquet and CSV formats (Windows safe)."""

    # --- Parquet ---
    if os.path.exists(output_parquet):
        shutil.rmtree(output_parquet, ignore_errors=True)
    df.write.mode("overwrite").parquet(output_parquet)
    print(f"[INFO] Saved DataFrame to Parquet: {output_parquet}")

    # --- CSV (via temp dir then rename) ---
    temp_csv_dir = output_csv + "_tmp_" + str(uuid.uuid4()).replace("-", "")
    if os.path.exists(temp_csv_dir):
        shutil.rmtree(temp_csv_dir, ignore_errors=True)

    df.coalesce(1).write.mode("overwrite").option("header", True).csv(temp_csv_dir)

    # Find the generated CSV file
    csv_file = None
    for f in os.listdir(temp_csv_dir):
        if f.endswith(".csv"):
            csv_file = os.path.join(temp_csv_dir, f)
            break

    # Remove old output and create fresh folder
    if os.path.exists(output_csv):
        shutil.rmtree(output_csv, ignore_errors=True)
    os.makedirs(output_csv, exist_ok=True)

    # Move CSV into final location with fixed name
    if csv_file:
        final_csv = os.path.join(output_csv, "countries_of_world.csv")
        shutil.move(csv_file, final_csv)
        print(f"[INFO] Saved DataFrame to CSV: {final_csv}")
    else:
        print("[ERROR] No CSV file generated by Spark")

    # Clean up temp dir
    shutil.rmtree(temp_csv_dir, ignore_errors=True)


# ----------------------------
# Ensure Output Directory
# ----------------------------
def ensure_output_dir(path: str) -> None:
    """Ensure that output directory exists for given path."""
    directory = os.path.dirname(path)
    if directory and not os.path.exists(directory):
        os.makedirs(directory, exist_ok=True)
        print(f"[INFO] Created directory: {directory}")
    else:
        print(f"[INFO] Directory already exists: {directory}")


# ----------------------------
# Prepare Countries DataFrame
# ----------------------------
def prepare_countries_df(df, country_map):
    """Prepare, clean, and normalize the Countries DataFrame."""

    # 1. Replace empty strings with NULL
    for c in df.columns:
        df = df.withColumn(
            c,
            when(trim(safe_col(c)) == "", None).otherwise(safe_col(c))
        )

    # 2. Clean column names
    for old_name in df.columns:
        new_name = old_name.strip().replace(",", ".")
        df = df.withColumnRenamed(old_name, new_name)

    # 3. Normalize country column
    if "Country" in df.columns:
        df = df.withColumnRenamed("Country", "CountryName")
    if "CountryName" in df.columns:
        df = df.withColumn("CountryName", trim(safe_col("CountryName")))

    # 4. Fix numeric formats
    numeric_cols = [
        "Pop. Density (per sq. mi.)",
        "Coastline (coast/area ratio)",
        "Net migration",
        "Infant mortality (per 1000 births)",
        "Literacy (%)",
        "Phones (per 1000)",
        "Arable (%)",
        "Crops (%)",
        "Other (%)",
        "Birthrate",
        "Deathrate",
        "Agriculture",
        "Industry",
        "Service",
    ]
    for col_name in numeric_cols:
        if col_name in df.columns:
            df = df.withColumn(
                col_name,
                when(trim(safe_col(col_name)) == "", None)
                .otherwise(regexp_replace(safe_col(col_name), ",", "."))
                .cast("double")
            )

    # 5. Run DQ Checks
    data_quality_checks(df, key_columns=["CountryName"], numeric_cols=numeric_cols)

    # 6. Drop duplicates
    before = df.count()
    df = df.dropDuplicates().cache()
    after = df.count()
    print(f"[INFO] Deduplicated entire rows: {before - after} duplicates removed")

    # 7. Normalize mapping
    if "countryname" in country_map.columns:
        country_map = country_map.withColumnRenamed("countryname", "CountryName")
    if "CountryName" in country_map.columns:
        country_map = country_map.withColumn("CountryName", trim(safe_col("CountryName")))

    # 8. Left join with trimming on both sides
    if "CountryName" in df.columns and "CountryName" in country_map.columns:
        before_join = df.count()
        df = df.withColumn("CountryName", trim(safe_col("CountryName"))) \
               .join(
                   country_map.select(trim(safe_col("CountryName")).alias("CountryName")),
                   on="CountryName",
                   how="left"   # <-- left join keeps all countries
               )
        after_join = df.count()
        print(f"[INFO] Left joined with mapping table (CountryName trimmed)")
        print(f"[INFO] Rows before join: {before_join}, after join: {after_join}, lost: {before_join - after_join}")

    # 9. Final select
    final_cols = [safe_col("CountryName")] + [safe_col(c) for c in numeric_cols if c in df.columns]
    df = df.select(*final_cols)

    return df


# ----------------------------
# Main Pipeline
# ----------------------------
def main():
    # 1. Initialize Spark
    spark = init_spark("CountriesOfWorldPipeline")
    print("Spark Version:", spark.version)

    # 2. Paths
    base_dir = r"C:\Users\Rupesh.shelar\data-engineer-test\datasets"
    input_file = os.path.join(base_dir, "countries", "countries of the world.csv")
    mapping_file = os.path.join(base_dir, "solution", "output", "country_code_mapping.parquet")
    output_parquet = os.path.join(base_dir, "solution", "output", "countries_of_world_parquet")
    output_csv = os.path.join(base_dir, "solution", "output", "countries_of_world_csv")
    ensure_output_dir(output_parquet)
    ensure_output_dir(output_csv)

    # 3. Load CSV
    df = (
        spark.read.option("header", True)
        .option("inferSchema", True)
        .csv(input_file)
    )

    # 4. Load mapping
    country_map = spark.read.parquet(mapping_file)

    # 5. Prepare DataFrame
    df = prepare_countries_df(df, country_map)

    # 6. Save outputs
    save_outputs(df, output_parquet, output_csv)

    # 7. Validate
    print("\n=== Data from Parquet Output ===")
    spark.read.parquet(output_parquet).show(5, truncate=False)

    print("\n=== Data from CSV Output ===")
    spark.read.option("header", True).csv(output_csv).show(5, truncate=False)

    # 8. Stop Spark
    spark.stop()


# ----------------------------
# Entry Point
# ----------------------------
if __name__ == "__main__":
    main()
